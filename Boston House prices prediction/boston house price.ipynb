{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.3 64-bit ('base': conda)",
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "2f9fbfe4749d0089818a674644891712019826e89c7f10430ca576b75f464ce7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# loading data of pre-loaded at keras\n",
    "from keras.datasets import boston_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break data into test and train. always try to split data 60:40, if data is messy you should do it 90:5:5 train,test and validation respectively\n",
    "(x_train,y_train),(x_test,y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of training data:  (404, 13)\nShape of training label:  (404,)\nShape of test label:  (102, 13)\nShape of test label:  (102,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of training data: ',x_train.shape)\n",
    "print('Shape of training label: ',y_train.shape)\n",
    "print('Shape of test label: ',x_test.shape)\n",
    "print('Shape of test label: ',y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data for validation\n",
    "x_val = x_train[300:,]\n",
    "y_val = y_train[300:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing numpy, keras sequential model and layer \n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(13,input_dim=13,kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dense(6,kernel_initializer ='normal',activation='relu'))\n",
    "model.add(Dense(1,kernel_initializer = 'normal',activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "model.compile(loss='mean_squared_error',optimizer='adam',metrics=['mean_absolute_percentage_error'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 404 samples, validate on 104 samples\n",
      "Epoch 1/3\n",
      "404/404 [==============================] - 2s 5ms/step - loss: 558.5813 - mean_absolute_percentage_error: 96.7365 - val_loss: 647.2076 - val_mean_absolute_percentage_error: 96.5179\n",
      "Epoch 2/3\n",
      "404/404 [==============================] - 0s 223us/step - loss: 550.2282 - mean_absolute_percentage_error: 95.7088 - val_loss: 637.9163 - val_mean_absolute_percentage_error: 95.5529\n",
      "Epoch 3/3\n",
      "404/404 [==============================] - 0s 262us/step - loss: 544.0551 - mean_absolute_percentage_error: 94.9547 - val_loss: 634.9229 - val_mean_absolute_percentage_error: 95.2324\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x152b3977550>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(x_train,y_train,batch_size=32,epochs=3,validation_data=(x_val,y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our losses goes to 95 % due to underfitting, so we should increase epoch size 3 to 10,15,30 with repect to loss "
   ]
  }
 ]
}